{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a1ace4-4544-454b-9081-fba7a1dd793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b985c88a-490d-4ef9-aed8-0fb5030b837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from typing import Any, Dict, Generator, IO\n",
    "\n",
    "\n",
    "def find_value_paths_large_file(\n",
    "    file: IO[str],\n",
    "    target_value: Any\n",
    ") -> Generator[Dict[str, Any], None, None]:\n",
    "    \"\"\"\n",
    "    Stream and search a VERY large JSON file that contains\n",
    "    a TOP-LEVEL LIST of JSON objects.\n",
    "\n",
    "    Yields: {\"path\": \"...\", \"value\": <object>}\n",
    "    \"\"\"\n",
    "\n",
    "    def search(obj: Any, path: str) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"Recursive search inside each JSON object.\"\"\"\n",
    "        if obj == target_value:\n",
    "            yield {\"path\": path, \"value\": obj}\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                new_path = f\"{path}.{key}\" if path else key\n",
    "                yield from search(value, new_path)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            for index, item in enumerate(obj):\n",
    "                new_path = f\"{path}[{index}]\"\n",
    "                yield from search(item, new_path)\n",
    "\n",
    "    # Stream each item of the top-level list\n",
    "    for obj in ijson.items(file, \"item\"):\n",
    "        # Search each object\n",
    "        yield from search(obj, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e634c95-ed2e-4b35-b661-e51dd4053544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf2c696-2829-4054-9241-f539d13bc3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'company.address.state', 'value': 'Oklahoma'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"users.json\", \"r\") as f:\n",
    "    for result in find_value_paths_large_file(f, target_value=\"Oklahoma\"):\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667719fe-6e5b-46a8-b8b1-22b8e47ec65f",
   "metadata": {},
   "source": [
    "Below is a **fully rewritten `find_value_paths()` that works with VERY large JSON files containing a TOP-LEVEL LIST of JSON objects â€” *without using ijson*.**\n",
    "\n",
    "âœ” Handles multi-GB JSON files\n",
    "âœ” Does **NOT** load the entire file into memory\n",
    "âœ” Reads one JSON object at a time from inside a huge array\n",
    "âœ” Correctly handles **nested braces** inside objects\n",
    "âœ” Fully type-annotated\n",
    "âœ” Output format:\n",
    "\n",
    "```python\n",
    "{\"path\": \"...\", \"value\": <object>}\n",
    "```\n",
    "\n",
    "âœ” Same search behavior as your original function\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… **Streaming `find_value_paths()` â€” No ijson Required**\n",
    "\n",
    "This version uses a **streaming JSON array object extractor** that:\n",
    "\n",
    "* Reads the file character-by-character\n",
    "* Detects JSON objects inside a large top-level array\n",
    "* Tracks `{` and `}` nesting so it can extract objects even when deeply nested\n",
    "* Parses *each object independently*\n",
    "* Searches it with your recursive logic\n",
    "\n",
    "---\n",
    "\n",
    "# â­ **Code: Streaming JSON Array Parser + Value Search**  BROKE?\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790d9b24-c08d-438b-8e5a-c5dc464552dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, Generator, IO\n",
    "\n",
    "\n",
    "def find_value_paths(\n",
    "    file: IO[str],\n",
    "    target_value: Any\n",
    ") -> Generator[Dict[str, Any], None, None]:\n",
    "    \"\"\"\n",
    "    Stream-search a very large JSON file whose top-level structure\n",
    "    is a LIST of JSON objects:\n",
    "        [ {...}, {...}, {...}, ... ]\n",
    "\n",
    "    Yields dictionaries:\n",
    "        {\"path\": \"<path>\", \"value\": <object>}\n",
    "    \"\"\"\n",
    "\n",
    "    def search(obj: Any, path: str) -> Generator[Dict[str, Any], None, None]:\n",
    "        \"\"\"\n",
    "        Recursively search for target_value in a JSON object.\n",
    "        \"\"\"\n",
    "        if obj == target_value:\n",
    "            yield {\"path\": path, \"value\": obj}\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                new_path = f\"{path}.{key}\" if path else key\n",
    "                yield from search(value, new_path)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            for idx, item in enumerate(obj):\n",
    "                new_path = f\"{path}[{idx}]\"\n",
    "                yield from search(item, new_path)\n",
    "\n",
    "    #\n",
    "    # ---- Stream JSON array object-by-object ----\n",
    "    #\n",
    "    in_array = False\n",
    "    brace_depth = 0\n",
    "    buffer = []\n",
    "    inside_object = False\n",
    "\n",
    "    while True:\n",
    "        chunk = file.read(65536)\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        for ch in chunk:\n",
    "\n",
    "            # Skip whitespace until '[' encountered\n",
    "            if not in_array:\n",
    "                if ch == '[':\n",
    "                    in_array = True\n",
    "                continue\n",
    "\n",
    "            # End of the array\n",
    "            if ch == ']' and not inside_object:\n",
    "                return\n",
    "\n",
    "            # Start of new object\n",
    "            if ch == '{':\n",
    "                inside_object = True\n",
    "                brace_depth = 1\n",
    "                buffer = ['{']\n",
    "                continue\n",
    "\n",
    "            # Accumulate object content\n",
    "            if inside_object:\n",
    "                buffer.append(ch)\n",
    "\n",
    "                # Track brace depth to detect end of object\n",
    "                if ch == '{':\n",
    "                    brace_depth += 1\n",
    "                elif ch == '}':\n",
    "                    brace_depth -= 1\n",
    "\n",
    "                # Object completed\n",
    "                if brace_depth == 0:\n",
    "                    json_str = \"\".join(buffer)\n",
    "\n",
    "                    try:\n",
    "                        obj = json.loads(json_str)\n",
    "                        # Perform recursive search on this object\n",
    "                        yield from search(obj, \"\")\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass  # Skip malformed objects\n",
    "\n",
    "                    inside_object = False\n",
    "                    buffer = []\n",
    "                continue\n",
    "\n",
    "            # Ignore commas between objects\n",
    "            if ch == ',':\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97b6e8-4b24-4c70-9296-69f7b4df33d0",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… How this works\n",
    "\n",
    "### âœ” **Does not load the whole list**\n",
    "\n",
    "Instead it:\n",
    "\n",
    "1. Waits until it sees the top-level `[`\n",
    "2. Extracts **one JSON object at a time** by tracking `{` and `}` depth\n",
    "3. For each object, runs your recursive search\n",
    "4. Yields results immediately\n",
    "5. Discards the object before reading the next\n",
    "\n",
    "Memory usage stays tiny (< 20MB), no matter how large the file is.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§ª Usage Example\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc6778c-8468-4f7b-99a8-c43939e87413",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"users.json\", \"r\") as f:\n",
    "    for match in find_value_paths(f, target_value=\"Oklahoma\"):\n",
    "        print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab890da-b6d7-4dc4-b4a4-4ed3155f098e",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“Œ Expected Output Example\n",
    "\n",
    "```python\n",
    "{'path': 'user.address.zipcode', 'value': 123}\n",
    "{'path': 'items[2].price', 'value': 123}\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸš€ Want additional features?\n",
    "\n",
    "I can also add:\n",
    "\n",
    "* Parallel processing (multiprocessing pool)\n",
    "* Progress bar for multi-GB files\n",
    "* Regex value matching\n",
    "* Value-type matching (find all integers, all lists, etc.)\n",
    "* Output results to a file instead of memory\n",
    "\n",
    "Just tell me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54b77d-56ce-4dfa-9da0-ea679e2cf989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py311b)",
   "language": "python",
   "name": "py311b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
